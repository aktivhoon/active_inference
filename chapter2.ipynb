{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active inference - Chapter 2\n",
    "\n",
    "## Posteiror probability\n",
    "$$\n",
    "P(x|y) = \\frac{P(x)P(y|x)}{P(y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_jump: 0.090\n",
      "posterior belief of frog: 0.900\n",
      "posterior belief of apple: 0.100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "keys = ['frog', 'apple']\n",
    "# index 0: frog, 1: apple\n",
    "prior_beliefs = np.array([0.1, 0.9])\n",
    "\n",
    "# 1st index 0: frog, 1: apple\n",
    "# 2nd index 0: jumps, 1: stays\n",
    "likelihoods = np.array([[0.81, 0.19],\n",
    "                        [0.01, 0.99]])\n",
    "\n",
    "# probability of jumping\n",
    "p_jump = prior_beliefs @ likelihoods[:, 0]\n",
    "print('p_jump: {:.3f}'.format(p_jump))\n",
    "# posterior belief\n",
    "posterior_beliefs = likelihoods[:, 0] * prior_beliefs / p_jump\n",
    "for i in range(2):\n",
    "    print('posterior belief of {}: {:.3f}'.format(keys[i], posterior_beliefs[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define surprise\n",
    "KL-Divergence can be defined as:\n",
    "$$\n",
    "D_{KL} \\left[ Q(x) \\| P(x) \\right] = \\mathbb{E}_{Q(x)} \\left[ \\text{ln } Q(x) - \\text{ln } P(x) \\right]\n",
    "$$\n",
    "\n",
    "Using KL-Divergence, we can quantify the Bayesian surprise in the example as:\n",
    "$$\n",
    "D_{KL} \\left[ P(x|y) \\| P(x) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL divergence: 1.758\n"
     ]
    }
   ],
   "source": [
    "def kl_divergence(p, q):\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "# KL divergence between prior and posterior\n",
    "kl = kl_divergence(prior_beliefs, posterior_beliefs)\n",
    "print('Surprise: {:.3f}'.format(kl))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing Variatonal Free Energy\n",
    "The exact Bayesian inference supporting perception and action is computationally intractable: model evidence ($P(y)$) and the posterior probability ($P(x|y)$) cannot be computed for two possible reasons:\n",
    "\n",
    "    1. For complex models, there may be many types of hidden states that all need marginalizing out (= making the probel computationally intractable)\n",
    "    2. The marginalization operation might require analytically intractable integrals\n",
    "\n",
    "Although posterior probability and model evidence are intractable quantaties, it can be computed efficiently with two approximated quantities: *approximate posterior* $Q$ and a *variational free energy*, $F$.\n",
    "\n",
    "$$\n",
    "F[Q, y] = -\\mathbb{E}_{Q(x)} \\left[ \\text{ln } P(y, x) \\right] - H \\left[ Q(x) \\right]\\\\\n",
    "%= D_{KL} \\left[ Q(x) \\| P(x) \\right] - \\mathbb{E}_{Q(x)} \\left[ \\text{ln } P(y|x) \\right] \\\\\n",
    "%= D_{KL} \\left[ Q(x) \\| P(x|y) \\right] - \\text{ln } P(y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
