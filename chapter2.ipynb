{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active inference - Chapter 2\n",
    "\n",
    "## Posteiror probability\n",
    "$$\n",
    "P(x|y) = \\frac{P(x)P(y|x)}{P(y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_jump: 0.090\n",
      "posterior belief of frog: 0.900\n",
      "posterior belief of apple: 0.100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "keys = ['frog', 'apple']\n",
    "# index 0: frog, 1: apple\n",
    "prior_beliefs = np.array([0.1, 0.9])\n",
    "\n",
    "# 1st index 0: frog, 1: apple\n",
    "# 2nd index 0: jumps, 1: stays\n",
    "likelihoods = np.array([[0.81, 0.19],\n",
    "                        [0.01, 0.99]])\n",
    "\n",
    "# probability of jumping\n",
    "p_jump = prior_beliefs @ likelihoods[:, 0]\n",
    "print('p_jump: {:.3f}'.format(p_jump))\n",
    "# posterior belief\n",
    "posterior_beliefs = likelihoods[:, 0] * prior_beliefs / p_jump\n",
    "for i in range(2):\n",
    "    print('posterior belief of {}: {:.3f}'.format(keys[i], posterior_beliefs[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define surprise\n",
    "KL-Divergence can be defined as:\n",
    "$$\n",
    "D_{KL} \\left[ Q(x) \\| P(x) \\right] = \\mathbb{E}_{Q(x)} \\left[ \\text{ln } Q(x) - \\text{ln } P(x) \\right]\n",
    "$$\n",
    "\n",
    "Using KL-Divergence, we can quantify the Bayesian surprise in the example as:\n",
    "$$\n",
    "D_{KL} \\left[ P(x|y) \\| P(x) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surprise: 1.758\n"
     ]
    }
   ],
   "source": [
    "def kl_divergence(p, q):\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "# KL divergence between prior and posterior\n",
    "kl = kl_divergence(prior_beliefs, posterior_beliefs)\n",
    "print('Surprise: {:.3f}'.format(kl))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing Variatonal Free Energy\n",
    "The exact Bayesian inference supporting perception and action is computationally intractable: model evidence ($P(y)$) and the posterior probability ($P(x|y)$) cannot be computed for two possible reasons:\n",
    "\n",
    "    1. For complex models, there may be many types of hidden states that all need marginalizing out (= making the probel computationally intractable)\n",
    "    2. The marginalization operation might require analytically intractable integrals\n",
    "\n",
    "Although posterior probability and model evidence are intractable quantaties, it can be computed efficiently with two approximated quantities: *approximate posterior* $Q$ and a *variational free energy*, $F$.\n",
    "\n",
    "$$\n",
    "F[Q, y] = -\\mathbb{E}_{Q(x)} \\left[ \\text{ln } P(y, x) \\right] - H \\left[ Q(x) \\right]\\\\\n",
    "%= D_{KL} \\left[ Q(x) \\| P(x) \\right] - \\mathbb{E}_{Q(x)} \\left[ \\text{ln } P(y|x) \\right] \\\\\n",
    "%= D_{KL} \\left[ Q(x) \\| P(x|y) \\right] - \\text{ln } P(y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing Expected Free Energy\n",
    "\n",
    "The expected free energy can be expressed as below:\n",
    "\n",
    "$$\n",
    "G(\\pi) = - \\mathbb{E}_{Q(\\tilde{x}, \\tilde{y} | \\pi)} \\left[ D_{KL} \\left[ Q(\\tilde{x} | \\tilde{y}, \\pi) \\| Q(\\tilde{x}|\\pi) \\right] \\right] - \\mathbb{E}_{Q(\\tilde{y}|\\pi)} \\left[ \\text{ln } P(\\tilde{y}|C) \\right]\n",
    "$$\n",
    "\n",
    "### Excercise Example - Projective Identification\n",
    "Let us say Dr. Smith is meeting a patient, Jane. \n",
    "\n",
    "Dr. Smith's internal state can be modeled as a binary variable with two possible states: neutral ($x_1$), and angry/critical ($x_2$).\n",
    "\n",
    "The observations will be modeled into two possible states: comforting words ($y_1$), and critical words ($y_2$).\n",
    "\n",
    "Let's assume the following probability distribution:\n",
    "* Prior probabilities of Dr. Smith's state given a policy $\\pi$: $Q(x|\\pi)$ = [0.3, 0.7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free energy of policy 1 (using PI): -0.072\n",
      "Free energy of policy 2 (not using PI): 0.080\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# prior belief of Dr. Smith's state (Q(x|pi), 2 x 1)\n",
    "prior_beliefs = np.array([[0.3],\n",
    "                          [0.7]])\n",
    "# likelihood1 of observations given Dr. Smith's state (P(y|x), 2 x 2)\n",
    "#                         y1   y2\n",
    "likelihoods1 = np.array([[0.7, 0.3],   # x1\n",
    "                        [0.05, 0.95]]) # x2\n",
    "\n",
    "# likelihood2 of observations given Dr. Smith's state (P(y|x), 2 x 2)\n",
    "#                         y1   y2\n",
    "likelihoods2 = np.array([[0.8, 0.2],   # x1\n",
    "                        [0.1, 0.9]])   # x2\n",
    "\n",
    "# joint probability of Dr. Smith's state and observations for policy 1 (Q(x,y|pi), 2 x 2)\n",
    "joint_prob1 = likelihoods1 * prior_beliefs\n",
    "# joint probability of Dr. Smith's state and observations for policy 2 (Q(x,y|pi), 2 x 2)\n",
    "joint_prob2 = likelihoods2 * prior_beliefs\n",
    "\n",
    "# posterior belief for policy 1 (Q(x|y,pi), 2 x 2)\n",
    "posterior_beliefs1 = joint_prob1 / np.sum(joint_prob1, axis=0)\n",
    "# posterior belief for policy 2 (Q(x|y,pi), 2 x 2)\n",
    "posterior_beliefs2 = joint_prob2 / np.sum(joint_prob2, axis=0)\n",
    "\n",
    "# marginal probability of observations for policy 1 (Q(y|pi), 2 x 1)\n",
    "marginal_prob1 = np.sum(joint_prob1, axis=0)\n",
    "# marginal probability of observations for policy 2 (Q(y|pi), 2 x 1)\n",
    "marginal_prob2 = np.sum(joint_prob2, axis=0)\n",
    "\n",
    "# Jane's preference\n",
    "pref = np.array([0.5, 0.5])\n",
    "\n",
    "# expected free energy for policy 1\n",
    "infogain1 = - np.sum(joint_prob1 * kl_divergence(posterior_beliefs1, prior_beliefs))\n",
    "pragval1 = - np.sum(marginal_prob1 * np.log(pref))\n",
    "free_energy1 = infogain1 + pragval1\n",
    "\n",
    "# expected free energy for policy 2\n",
    "infogain2 = - np.sum(joint_prob2 * kl_divergence(posterior_beliefs2, prior_beliefs))\n",
    "pragval2 = - np.sum(marginal_prob2 * np.log(pref))\n",
    "free_energy2 = infogain2 + pragval2\n",
    "\n",
    "groups = ['PI', 'not PI']\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.bar(groups, [infogain1, infogain2], label='Information gain')\n",
    "ax.bar(groups, [pragval1, pragval2], label='Pragmatic value', bottom=[infogain1, infogain2])\n",
    "\n",
    "print(\"Free energy of policy 1 (using PI): {:.3f}\".format(free_energy1))\n",
    "print(\"Free energy of policy 2 (not using PI): {:.3f}\".format(free_energy2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
